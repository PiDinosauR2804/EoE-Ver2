{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "\n",
    "class BaseData:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.label_list = self._read_labels()\n",
    "        self.id2label, self.label2id = [], {}\n",
    "        self.label2task_id = {}\n",
    "        self.train_data, self.val_data, self.test_data = None, None, None\n",
    "\n",
    "    def _read_labels(self):\n",
    "        \"\"\"\n",
    "        :return: only return the label name, in order to set label index from 0 more conveniently.\n",
    "        \"\"\"\n",
    "        id2label = json.load(open(os.path.join(self.args.data_path, self.args.dataset_name, 'id2label.json')))\n",
    "        return id2label\n",
    "\n",
    "    def read_and_preprocess(self, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def add_labels(self, cur_labels, task_id):\n",
    "        for c in cur_labels:\n",
    "            if c not in self.id2label:\n",
    "                self.id2label.append(c)\n",
    "                self.label2id[c] = len(self.label2id)\n",
    "                self.label2task_id[self.label2id[c]] = task_id\n",
    "\n",
    "    def filter(self, labels, split='train'):\n",
    "        if not isinstance(labels, list):\n",
    "            labels = [labels]\n",
    "        split = split.lower()\n",
    "        res = []\n",
    "        for label in labels:\n",
    "            if split == 'train':\n",
    "                if self.args.debug:\n",
    "                    res += copy.deepcopy(self.train_data[label])[:10]\n",
    "                else:\n",
    "                    res += copy.deepcopy(self.train_data[label])\n",
    "            elif split in ['dev', 'val']:\n",
    "                if self.args.debug:\n",
    "                    res += copy.deepcopy(self.val_data[label])[:10]\n",
    "                else:\n",
    "                    res += copy.deepcopy(self.val_data[label])\n",
    "            elif split == 'test':\n",
    "                if self.args.debug:\n",
    "                    res += copy.deepcopy(self.test_data[label])[:10]\n",
    "                else:\n",
    "                    res += copy.deepcopy(self.test_data[label])\n",
    "        for idx in range(len(res)):\n",
    "            res[idx][\"labels\"] = self.label2id[res[idx][\"labels\"]]\n",
    "        return res\n",
    "\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            res = []\n",
    "            for key in data.keys():\n",
    "                res += data[key]\n",
    "            data = res\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # cur_data = self.data[idx]\n",
    "        # cur_data[\"idx\"] = idx\n",
    "        # mask_head = True if random.random() > 0.5 else False\n",
    "        # input_ids, attention_mask, subject_start_pos, object_start_pos = mask_entity(cur_data[\"input_ids\"], mask_head)\n",
    "        # augment_data = {\n",
    "        #     \"input_ids\": input_ids,\n",
    "        #     \"attention_mask\": attention_mask,\n",
    "        #     \"subject_start_pos\": subject_start_pos,\n",
    "        #     \"object_start_pos\": object_start_pos,\n",
    "        #     \"labels\": cur_data[\"labels\"],\n",
    "        #     \"idx\": idx\n",
    "        # }\n",
    "        # return [cur_data, augment_data]\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FewRelData(BaseData):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.entity_markers = [\"[E11]\", \"[E12]\", \"[E21]\", \"[E22]\"]\n",
    "\n",
    "    def remove_entity_markers(self, input_ids):\n",
    "        ans = []\n",
    "        entity_pos = {}\n",
    "        for c in input_ids:\n",
    "            if c not in [30522, 30523, 30524, 30525]:\n",
    "                ans.append(c)\n",
    "            else:\n",
    "                if c % 2 == 0:\n",
    "                    entity_pos[c] = len(ans)\n",
    "                else:\n",
    "                    entity_pos[c] = len(ans) - 1\n",
    "        return ans, entity_pos[30522], entity_pos[30523], entity_pos[30524], entity_pos[30525]\n",
    "\n",
    "    def preprocess(self, raw_data, tokenizer):\n",
    "        subject_start_marker = tokenizer.convert_tokens_to_ids(self.entity_markers[0])\n",
    "        object_start_marker = tokenizer.convert_tokens_to_ids(self.entity_markers[2])\n",
    "        subject_end_marker = tokenizer.convert_tokens_to_ids(self.entity_markers[1])\n",
    "        object_end_marker = tokenizer.convert_tokens_to_ids(self.entity_markers[3])\n",
    "        res = []\n",
    "        result = tokenizer(raw_data['sentence'])\n",
    "        for idx in range(len(raw_data['sentence'])):\n",
    "            subject_marker_st = result['input_ids'][idx].index(subject_start_marker)\n",
    "            object_marker_st = result['input_ids'][idx].index(object_start_marker)\n",
    "            subject_marker_ed = result['input_ids'][idx].index(subject_end_marker)\n",
    "            object_marker_ed = result['input_ids'][idx].index(object_end_marker)\n",
    "            input_ids = result['input_ids'][idx]\n",
    "            sentence = copy.deepcopy(raw_data['sentence'][idx])\n",
    "            for c in self.entity_markers:\n",
    "                sentence = sentence.replace(c, '')\n",
    "            sentence = sentence.replace('  ', ' ')\n",
    "            # prompt_input_ids, mask_pos = self.get_prompt_input_ids(input_ids)\n",
    "            input_ids_without_marker, subject_st, subject_ed, object_st, object_ed = \\\n",
    "                self.remove_entity_markers(input_ids)\n",
    "            ins = {\n",
    "                'sentence': sentence,\n",
    "                'input_ids': input_ids,  # default: add marker to the head entity and tail entity\n",
    "                'subject_marker_st': subject_marker_st,\n",
    "                'object_marker_st': object_marker_st,\n",
    "                'labels': raw_data['labels'][idx],\n",
    "                'input_ids_without_marker': input_ids_without_marker,\n",
    "                'subject_st': subject_st,\n",
    "                'subject_ed': subject_ed,\n",
    "                'object_st': object_st,\n",
    "                'object_ed': object_ed,\n",
    "            }\n",
    "            if hasattr(self.args, 'columns'):\n",
    "                columns = self.args.columns\n",
    "                ins = {k: v for k, v in ins.items() if k in columns}\n",
    "            res.append(ins)\n",
    "        return res\n",
    "\n",
    "    def read_and_preprocess(self, tokenizer, seed=None):\n",
    "        raw_data = json.load(open(os.path.join(self.args.data_path, self.args.dataset_name, 'data_with_marker.json')))\n",
    "\n",
    "        train_data = {}\n",
    "        val_data = {}\n",
    "        test_data = {}\n",
    "\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "\n",
    "        for label in tqdm(raw_data.keys(), desc=\"Load FewRel data\"):\n",
    "            cur_data = raw_data[label]\n",
    "            random.shuffle(cur_data)\n",
    "            train_raw_data = {\"sentence\": [], \"labels\": []}\n",
    "            val_raw_data = {\"sentence\": [], \"labels\": []}\n",
    "            test_raw_data = {\"sentence\": [], \"labels\": []}\n",
    "            for idx, sample in enumerate(cur_data):\n",
    "                sample[\"tokens\"] = ' '.join(sample[\"tokens\"])\n",
    "                sample[\"relation\"] = sample[\"relation\"]\n",
    "                if idx < 420:\n",
    "                    train_raw_data[\"sentence\"].append(sample[\"tokens\"])\n",
    "                    train_raw_data[\"labels\"].append(sample[\"relation\"])\n",
    "                elif idx < 420 + 140:\n",
    "                    val_raw_data[\"sentence\"].append(sample[\"tokens\"])\n",
    "                    val_raw_data[\"labels\"].append(sample[\"relation\"])\n",
    "                else:\n",
    "                    test_raw_data[\"sentence\"].append(sample[\"tokens\"])\n",
    "                    test_raw_data[\"labels\"].append(sample[\"relation\"])\n",
    "\n",
    "            train_data[label] = self.preprocess(train_raw_data, tokenizer)\n",
    "            val_data[label] = self.preprocess(val_raw_data, tokenizer)\n",
    "            test_data[label] = self.preprocess(test_raw_data, tokenizer)\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        use_fast=True,\n",
    "        additional_special_tokens=[\"[E11]\", \"[E12]\", \"[E21]\", \"[E22]\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load FewRel data: 100%|██████████| 80/80 [00:02<00:00, 31.30it/s]\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_path = \"datasets\"  # đường dẫn đến dữ liệu của bạn\n",
    "        self.dataset_name = \"FewRel\"  # tên dataset của bạn\n",
    "\n",
    "# Khởi tạo đối tượng args\n",
    "args = Args()\n",
    "\n",
    "data = FewRelData(args=args)\n",
    "data.read_and_preprocess(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, samples in data.train_data.items():\n",
    "    print(f\"Label: {label}\")\n",
    "    for sample in samples:\n",
    "        print(sample)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class PeftFeatureExtractor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.device = config.device\n",
    "        self.dataset = config.dataset_name\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(config.model_name_or_path)\n",
    "        self.bert.resize_token_embeddings(\n",
    "            self.bert.config.vocab_size + config.additional_special_tokens_len\n",
    "        )\n",
    "\n",
    "        self.origin_bert = None\n",
    "        self.peft_bert = None\n",
    "        self.peft_type = config.peft_type if hasattr(config, \"peft_type\") else None\n",
    "        self.peft_init = config.peft_init if hasattr(config, \"peft_init\") else None\n",
    "\n",
    "        self.prompts = nn.ParameterList()\n",
    "        self.pre_seq_len = config.pre_seq_len if hasattr(config, \"pre_seq_len\") else None\n",
    "        self.n_layer = self.bert.config.num_hidden_layers\n",
    "        self.n_head = self.bert.config.num_attention_heads\n",
    "        self.n_embd = self.bert.config.hidden_size // self.bert.config.num_attention_heads\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "\n",
    "        if config.task_name == \"RelationExtraction\":\n",
    "            self.extract_mode = \"entity_marker\"\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if config.frozen:\n",
    "            logger.info(\"freeze the parameters of the pretrained language model.\")\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.origin_bert = copy.deepcopy(self.bert)\n",
    "            for param in self.origin_bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def add_adapter(self, task_id):\n",
    "        # Todo: support more peft types like prefix tuning, prompt tuning and so on.\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.FEATURE_EXTRACTION, inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1,\n",
    "            target_modules=[\"key\", \"query\", \"value\"],\n",
    "        )\n",
    "        adapter_name = f\"task-{task_id}\"\n",
    "        self.peft_bert = get_peft_model(copy.deepcopy(self.bert), peft_config, adapter_name)\n",
    "        self.peft_bert.print_trainable_parameters()\n",
    "        logger.info(f\"inject {self.peft_type} into the pretrain model, name is {adapter_name}\")\n",
    "\n",
    "    def save_and_load_all_adapters(self, task_id, save_dir, save=True):\n",
    "        if save:\n",
    "            self.peft_bert.save_pretrained(save_dir)\n",
    "        self.peft_bert = PeftModel.from_pretrained(\n",
    "            copy.deepcopy(self.bert),\n",
    "            f\"{save_dir}/task-0\",\n",
    "            adapter_name=\"task-0\"\n",
    "        )\n",
    "        for i in range(1, task_id + 1):\n",
    "            self.peft_bert.load_adapter(f\"{save_dir}/task-{i}\", adapter_name=f\"task-{i}\")\n",
    "\n",
    "    def load_adapter(self, task_id):\n",
    "        adapter_name = f\"task-{task_id}\"\n",
    "        self.peft_bert.set_adapter(adapter_name)\n",
    "\n",
    "    def get_prompts_by_indices(self, indices, attention_mask):\n",
    "        batch_size, _ = attention_mask.size()\n",
    "\n",
    "        prompt_attention_mask = torch.ones(batch_size, self.pre_seq_len, dtype=torch.long, device=self.device)\n",
    "        attention_mask = torch.cat([prompt_attention_mask, attention_mask], dim=1)\n",
    "\n",
    "        prompt = torch.stack([self.prompts[idx] for idx in indices])\n",
    "        past_key_values = None\n",
    "        \n",
    "        return past_key_values, attention_mask, prompt\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids,\n",
    "            inputs_embeds=None,\n",
    "            attention_mask=None,\n",
    "            extract_mode=None,\n",
    "            use_origin=False,\n",
    "            indices=None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        batch_size, _ = input_ids.size()\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = input_ids != 0\n",
    "\n",
    "        if use_origin:\n",
    "            outputs = self.origin_bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "        elif self.peft_type is not None and indices is not None:\n",
    "            if self.peft_type == \"lora\":\n",
    "                self.load_adapter(indices[0])\n",
    "                outputs = self.peft_bert(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "            elif self.peft_type == \"prefix\":\n",
    "                past_key_values, attention_mask, _ = self.get_prompts_by_indices(indices, attention_mask)\n",
    "                outputs = self.bert(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    past_key_values=past_key_values\n",
    "                )\n",
    "            elif self.peft_type == \"prompt\":\n",
    "                _, attention_mask, prompt = self.get_prompts_by_indices(indices, attention_mask)\n",
    "                prompt_len = prompt.shape[1]\n",
    "                inputs_embeds = self.bert.embeddings.word_embeddings(input_ids)\n",
    "                inputs_embeds = torch.cat([prompt, inputs_embeds], dim=1)  # (batch, prompt_len + sent_len, dim)\n",
    "                outputs = self.bert(\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "                outputs[0] = outputs[0][:, prompt_len:, :]\n",
    "                attention_mask = attention_mask[:, prompt_len:]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            # only for tuning\n",
    "            outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                past_key_values=kwargs[\"past_key_values\"] if \"past_key_values\" in kwargs else None,\n",
    "            )\n",
    "\n",
    "        extract_mode = extract_mode if extract_mode is not None else self.extract_mode\n",
    "        # different feature extraction modes\n",
    "        if extract_mode == \"cls\":\n",
    "            hidden_states = outputs[1]  # (batch, dim)\n",
    "        elif extract_mode == \"mean_pooling\":\n",
    "            # (batch, dim)\n",
    "            hidden_states = torch.sum(outputs[0] * attention_mask.unsqueeze(-1), dim=1) / \\\n",
    "                            torch.sum(attention_mask, dim=1).unsqueeze(-1)\n",
    "        elif extract_mode == \"mask\":\n",
    "            mask_pos = kwargs[\"mask_pos\"]\n",
    "            last_hidden_states = outputs[0]\n",
    "            idx = torch.arange(last_hidden_states.size(0)).to(last_hidden_states.device)\n",
    "            hidden_states = last_hidden_states[idx, mask_pos]\n",
    "        elif extract_mode == \"entity\":\n",
    "            last_hidden_states = outputs[0]\n",
    "            subj_st, subj_ed = kwargs[\"subject_st\"], kwargs[\"subject_ed\"]\n",
    "            obj_st, obj_ed = kwargs[\"object_st\"], kwargs[\"object_ed\"]\n",
    "            hidden_states = []\n",
    "            for idx in range(last_hidden_states.size(0)):\n",
    "                subj = last_hidden_states[idx][subj_st[idx]: subj_ed[idx] + 1]\n",
    "                obj = last_hidden_states[idx][obj_st[idx]: obj_ed[idx] + 1]\n",
    "                subj = subj.mean(0)\n",
    "                obj = obj.mean(0)\n",
    "                hidden_states.append(torch.cat([subj, obj]))\n",
    "            hidden_states = torch.stack(hidden_states, dim=0)\n",
    "        elif extract_mode == \"entity_marker\":\n",
    "            subject_start_pos = kwargs[\"subject_marker_st\"]\n",
    "            object_start_pos = kwargs[\"object_marker_st\"]\n",
    "            last_hidden_states = outputs[0]\n",
    "            idx = torch.arange(last_hidden_states.size(0)).to(last_hidden_states.device)\n",
    "            ss_emb = last_hidden_states[idx, subject_start_pos]\n",
    "            os_emb = last_hidden_states[idx, object_start_pos]\n",
    "            hidden_states = torch.cat([ss_emb, os_emb], dim=-1)  # (batch, 2 * dim)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "\n",
    "\n",
    "class ExpertOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: Optional[torch.FloatTensor] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "class ExpertModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.device = config.device\n",
    "\n",
    "        self.feature_extractor = PeftFeatureExtractor(config)\n",
    "        self.hidden_size = self.feature_extractor.bert.config.hidden_size\n",
    "\n",
    "        self.num_old_labels = 0\n",
    "        self.num_labels = 0\n",
    "\n",
    "        self.classifier_hidden_size = self.feature_extractor.bert.config.hidden_size\n",
    "        if config.task_name == \"RelationExtraction\":\n",
    "            self.classifier_hidden_size = 2 * self.feature_extractor.bert.config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Linear(self.classifier_hidden_size, self.num_labels)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def new_task(self, num_labels):\n",
    "        self.num_old_labels = self.num_labels\n",
    "        self.num_labels += num_labels\n",
    "        w = self.classifier.weight.data.clone()\n",
    "        b = self.classifier.bias.data.clone()\n",
    "        self.classifier = nn.Linear(self.classifier_hidden_size, self.num_labels, device=self.device)\n",
    "        self.classifier.weight.data[:self.num_old_labels] = w\n",
    "        self.classifier.bias.data[:self.num_old_labels] = b\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "        hidden_states = self.feature_extractor(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return ExpertOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=hidden_states,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Union\n",
    "\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "\n",
    "class CustomCollatorWithPadding:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def pad_to_same_length(self, batch_data):\n",
    "        if isinstance(batch_data[0], int):\n",
    "            if self.return_tensors == \"pt\":\n",
    "                return torch.LongTensor(batch_data)\n",
    "            else:\n",
    "                return batch_data\n",
    "        max_length = max([len(c) for c in batch_data])\n",
    "        ans = []\n",
    "        for ins in batch_data:\n",
    "            ins = ins + [0] * (max_length - len(ins))\n",
    "            ans.append(ins)\n",
    "        if self.return_tensors == \"pt\":\n",
    "            return torch.LongTensor(ans)\n",
    "        else:\n",
    "            return ans\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        batch_keys = features[0].keys()\n",
    "        batch = {k: [] for k in batch_keys}\n",
    "        for ins in features:\n",
    "            for k in batch_keys:\n",
    "                batch[k].append(ins[k])\n",
    "        for k in batch_keys:\n",
    "            batch[k] = self.pad_to_same_length(batch[k])\n",
    "        return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import relation_data_augmentation, CustomCollatorWithPadding\n",
    "\n",
    "class ExpertTrainer:\n",
    "    def __init__(self, args, **kwargs):\n",
    "        self.optimizer = None\n",
    "        self.task_idx = 0\n",
    "        self.args = args\n",
    "\n",
    "    def run(self, data, model, tokenizer, label_order, seed=None):\n",
    "        if seed is not None:\n",
    "            set_seed(seed)\n",
    "        default_data_collator = CustomCollatorWithPadding(tokenizer)\n",
    "\n",
    "        seen_labels = []\n",
    "        all_cur_acc = [0] * self.args.num_tasks\n",
    "        all_total_acc = [0] * self.args.num_tasks\n",
    "        all_total_hit = [0] * self.args.num_tasks\n",
    "        marker_ids = tuple([tokenizer.convert_tokens_to_ids(c) for c in self.args.additional_special_tokens])\n",
    "        logger.info(f\"marker ids: {marker_ids}\")\n",
    "        for task_idx in range(self.args.num_tasks):\n",
    "            self.task_idx = task_idx\n",
    "            cur_labels = [data.label_list[c] for c in label_order[task_idx]]\n",
    "            data.add_labels(cur_labels, task_idx)\n",
    "            seen_labels += cur_labels\n",
    "\n",
    "            logger.info(f\"***** Task-{task_idx + 1} *****\")\n",
    "            logger.info(f\"Current classes: {' '.join(cur_labels)}\")\n",
    "\n",
    "            train_data = data.filter(cur_labels, \"train\")\n",
    "            # data augmentation\n",
    "            num_train_labels = len(cur_labels)\n",
    "            train_data, num_train_labels = relation_data_augmentation(\n",
    "                train_data, len(seen_labels), copy.deepcopy(data.id2label), marker_ids, self.args.augment_type\n",
    "            )\n",
    "            train_dataset = BaseDataset(train_data)\n",
    "\n",
    "            model.new_task(num_train_labels)\n",
    "\n",
    "            self.train(\n",
    "                model=model,\n",
    "                train_dataset=train_dataset,\n",
    "                data_collator=default_data_collator\n",
    "            )\n",
    "            cur_test_data = data.filter(cur_labels, 'test')\n",
    "            cur_test_dataset = BaseDataset(cur_test_data)\n",
    "            cur_result = self.eval(\n",
    "                model=model,\n",
    "                eval_dataset=cur_test_dataset,\n",
    "                data_collator=default_data_collator,\n",
    "                seen_labels=seen_labels,\n",
    "            )\n",
    "\n",
    "            os.makedirs(self.args.save_model_dir, exist_ok=True)\n",
    "            save_model_name = f\"{self.args.dataset_name}_{seed}_{self.args.augment_type}.pth\"\n",
    "            save_model_path = os.path.join(self.args.save_model_dir, save_model_name)\n",
    "            logger.info(f\"save expert model to {save_model_path}\")\n",
    "            self.save_model(model, save_model_path)\n",
    "\n",
    "            all_cur_acc[self.task_idx] = cur_result\n",
    "            all_total_acc[self.task_idx] = cur_result\n",
    "            all_total_hit[self.task_idx] = 1\n",
    "            # only for the first task\n",
    "            if self.task_idx == 0:\n",
    "                break\n",
    "\n",
    "        return {\n",
    "            \"cur_acc\": all_cur_acc,\n",
    "            \"total_acc\": all_total_acc,\n",
    "            \"total_hit\": all_total_hit,\n",
    "        }\n",
    "\n",
    "    def train(self, model, train_dataset, data_collator):\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=data_collator\n",
    "        )\n",
    "        len_dataloader = len(train_dataloader)\n",
    "        num_examples = len(train_dataset)\n",
    "        max_steps = len_dataloader * self.args.num_train_epochs\n",
    "\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(f\"  Num examples = {num_examples}\")\n",
    "        logger.info(f\"  Num Epochs = {self.args.num_train_epochs}\")\n",
    "        logger.info(f\"  Train batch size = {self.args.train_batch_size}\")\n",
    "        logger.info(f\"  Total optimization steps = {max_steps}\")\n",
    "\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if 'feature_extractor' in n and not any(nd in n for nd in no_decay)],\n",
    "             'lr': self.args.learning_rate, 'weight_decay': 1e-2},\n",
    "            {'params': [p for n, p in model.named_parameters() if 'feature_extractor' in n and any(nd in n for nd in no_decay)],\n",
    "             'lr': self.args.learning_rate, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if 'feature_extractor' not in n and not any(nd in n for nd in no_decay)],\n",
    "             'lr': self.args.classifier_learning_rate, 'weight_decay': 1e-2},\n",
    "            {'params': [p for n, p in model.named_parameters() if 'feature_extractor' not in n and any(nd in n for nd in no_decay)],\n",
    "             'lr': self.args.classifier_learning_rate, 'weight_decay': 0.0},\n",
    "        ]\n",
    "        self.optimizer = AdamW(parameters)\n",
    "\n",
    "        progress_bar = tqdm(range(max_steps))\n",
    "\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         print(name)\n",
    "\n",
    "        for epoch in range(self.args.num_train_epochs):\n",
    "            model.train()\n",
    "            for step, inputs in enumerate(train_dataloader):\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                inputs = {k: v.to(self.args.device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), self.args.max_grad_norm)\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                progress_bar.update(1)\n",
    "                progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval(self, model, eval_dataset, data_collator, seen_labels):\n",
    "        eval_dataloader = DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=self.args.eval_batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "\n",
    "        len_dataloader = len(eval_dataloader)\n",
    "        num_examples = len(eval_dataset)\n",
    "\n",
    "        logger.info(\"***** Running evaluating *****\")\n",
    "        logger.info(f\"  Num examples = {num_examples}\")\n",
    "        logger.info(f\"  Eval batch size = {self.args.eval_batch_size}\")\n",
    "\n",
    "        progress_bar = tqdm(range(len_dataloader))\n",
    "\n",
    "        golds = []\n",
    "        preds = []\n",
    "\n",
    "        model.eval()\n",
    "        for step, inputs in enumerate(eval_dataloader):\n",
    "            labels = inputs.pop('labels')\n",
    "            inputs = {k: v.to(self.args.device) for k, v in inputs.items()}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predicts = logits.max(dim=-1)[1]\n",
    "\n",
    "            predicts = predicts.cpu().tolist()\n",
    "            labels = labels.cpu().tolist()\n",
    "            golds.extend(predicts)\n",
    "            preds.extend(labels)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "        progress_bar.close()\n",
    "\n",
    "        micro_f1 = metrics.f1_score(golds, preds, average='micro')\n",
    "        logger.info(\"Micro F1 {}\".format(micro_f1))\n",
    "\n",
    "        return micro_f1\n",
    "\n",
    "    def save_model(self, model, save_path):\n",
    "        bert_state_dict = model.feature_extractor.bert.state_dict()\n",
    "        linear_state_dict = model.classifier.state_dict()\n",
    "        torch.save({\n",
    "            \"model\": bert_state_dict,\n",
    "            \"linear\": linear_state_dict,\n",
    "        }, save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_result = trainer.run(\n",
    "    data=data,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    label_order=task_seq,\n",
    "    seed=exp_seed\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
